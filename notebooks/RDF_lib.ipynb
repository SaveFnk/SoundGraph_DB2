{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spotify and YouTube dataset population\n",
    "\n",
    "This notebook outlines the steps to create an RDF dataset based on the SoundGraph ontology, from the data import to RDF triple export in Turtle format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:10:40.494586300Z",
     "start_time": "2023-12-02T14:10:40.230777300Z"
    }
   },
   "outputs": [],
   "source": [
    "# required library\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:10:43.947718800Z",
     "start_time": "2023-12-02T14:10:43.942716600Z"
    }
   },
   "outputs": [],
   "source": [
    "# csv files path\n",
    "dataset_path='../data/Spotify_Youtube.csv'\n",
    "spotify_artist_path = '../data/Artists.csv'\n",
    "spotify_artist_info_path = '../data/Artist_info.csv'\n",
    "spotify_album_path = '../data/Album_info.csv'\n",
    "wikidata_artists_path = '../data/wikidata_artists.csv'\n",
    "wikidata_award_statements_path = '../data/wikidata_award_statements.csv'\n",
    "wikidata_awards_path = '../data/wikidata_awards.csv'\n",
    "youtube_api_channels_path = '../data/youtubeapi_channels_complete.csv'\n",
    "# target folder where to save the output\n",
    "targetFolder = '../rdf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:10:50.083460600Z",
     "start_time": "2023-12-02T14:10:48.798139100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20718 entries, 0 to 20717\n",
      "Data columns (total 28 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        20718 non-null  int64  \n",
      " 1   Artist            20718 non-null  object \n",
      " 2   Url_spotify       20718 non-null  object \n",
      " 3   Track             20718 non-null  object \n",
      " 4   Album             20718 non-null  object \n",
      " 5   Album_type        20718 non-null  object \n",
      " 6   Uri               20718 non-null  object \n",
      " 7   Danceability      20716 non-null  float64\n",
      " 8   Energy            20716 non-null  float64\n",
      " 9   Key               20716 non-null  float64\n",
      " 10  Loudness          20716 non-null  float64\n",
      " 11  Speechiness       20716 non-null  float64\n",
      " 12  Acousticness      20716 non-null  float64\n",
      " 13  Instrumentalness  20716 non-null  float64\n",
      " 14  Liveness          20716 non-null  float64\n",
      " 15  Valence           20716 non-null  float64\n",
      " 16  Tempo             20716 non-null  float64\n",
      " 17  Duration_ms       20716 non-null  float64\n",
      " 18  Url_youtube       20248 non-null  object \n",
      " 19  Title             20248 non-null  object \n",
      " 20  Channel           20248 non-null  object \n",
      " 21  Views             20248 non-null  float64\n",
      " 22  Likes             20177 non-null  float64\n",
      " 23  Comments          20149 non-null  float64\n",
      " 24  Description       19842 non-null  object \n",
      " 25  Licensed          20248 non-null  object \n",
      " 26  official_video    20248 non-null  object \n",
      " 27  Stream            20142 non-null  float64\n",
      "dtypes: float64(15), int64(1), object(12)\n",
      "memory usage: 4.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# load the songs \n",
    "dataset = pd.read_csv(dataset_path, sep=',')\n",
    "# preprocessing of URLs, needed to get uri\n",
    "#dataset['Url_spotify'] = dataset['Url_spotify'].fillna('_').apply(lambda uri: uri.split(':')[-1]) \n",
    "#dataset['Uri'] = dataset['Uri'].fillna('_').apply(lambda uri: uri.split(':')[-1]) \n",
    "#dataset['Album'] = dataset['Album'].fillna('_').apply(lambda uri: uri.split(':')[-1]) \n",
    "#dataset['Url_youtube'] = dataset['Url_youtube'].fillna('_').apply(lambda uri: uri.split('?v=')[-1]) \n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:10:54.531861Z",
     "start_time": "2023-12-02T14:10:54.466863100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2079 entries, 0 to 2078\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  2079 non-null   int64 \n",
      " 1   Artist      2079 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 32.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# load the artists present\n",
    "spotify_artist = pd.read_csv(spotify_artist_path, sep=',')\n",
    "spotify_artist.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:10:56.715189600Z",
     "start_time": "2023-12-02T14:10:56.472189900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2079 entries, 0 to 2078\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Artist      2079 non-null   object\n",
      " 1   Followers   2079 non-null   int64 \n",
      " 2   Genres      2079 non-null   object\n",
      " 3   Popularity  2079 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 65.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# load spotify artists information\n",
    "spotify_artist_info = pd.read_csv(spotify_artist_info_path, sep=',')\n",
    "spotify_artist_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:10:58.314333700Z",
     "start_time": "2023-12-02T14:10:57.427554800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20718 entries, 0 to 20717\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Id                20718 non-null  object\n",
      " 1   Album             20641 non-null  object\n",
      " 2   Total_tracks      20718 non-null  int64 \n",
      " 3   Release_date      20718 non-null  object\n",
      " 4   Available_market  20718 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 809.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# load spotify album information\n",
    "spotify_album = pd.read_csv(spotify_album_path, sep=',')\n",
    "spotify_album.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:10:59.059643Z",
     "start_time": "2023-12-02T14:10:58.996641200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2079 entries, 0 to 2078\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Artist         2079 non-null   object\n",
      " 1   Url_spotify    2079 non-null   object\n",
      " 2   artistLabel    2079 non-null   object\n",
      " 3   websiteLabel   2079 non-null   object\n",
      " 4   start          2079 non-null   object\n",
      " 5   end            2079 non-null   object\n",
      " 6   dissolved      2079 non-null   object\n",
      " 7   country_codes  2079 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 130.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# load the wikidata artists information\n",
    "wikidata_artists = pd.read_csv(wikidata_artists_path, sep=',')\n",
    "wikidata_artists.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:11:02.656078800Z",
     "start_time": "2023-12-02T14:11:02.589076700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1002 entries, 0 to 1001\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   artist_spotify_id  1002 non-null   object\n",
      " 1   award_id           1002 non-null   int64 \n",
      " 2   award_year         1002 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 23.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# load the wikidata award statements information\n",
    "wikidata_award_statements = pd.read_csv(wikidata_award_statements_path, sep=',')\n",
    "wikidata_award_statements.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:11:03.483592900Z",
     "start_time": "2023-12-02T14:11:03.423593900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 193 entries, 0 to 192\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   award_id        193 non-null    int64 \n",
      " 1   award_name      193 non-null    object\n",
      " 2   award_type      193 non-null    object\n",
      " 3   award_category  118 non-null    object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 6.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# load the wikidata awards information\n",
    "wikidata_awards = pd.read_csv(wikidata_awards_path,sep=',')\n",
    "wikidata_awards.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:11:04.352567500Z",
     "start_time": "2023-12-02T14:11:04.155568800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6715 entries, 0 to 6714\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   channelId           6715 non-null   object\n",
      " 1   title               6715 non-null   object\n",
      " 2   channelDescription  4241 non-null   object\n",
      " 3   viewCount           6715 non-null   object\n",
      " 4   subscriberCount     6715 non-null   object\n",
      " 5   videoCount          6715 non-null   object\n",
      " 6   error               6715 non-null   int64 \n",
      " 7   originalChannel     6715 non-null   object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 419.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# load the youtube channels information\n",
    "youtube_api_channels = pd.read_csv(youtube_api_channels_path, sep=',')\n",
    "youtube_api_channels.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### DataFrames for obj prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:11:11.067787100Z",
     "start_time": "2023-12-02T14:11:11.029802400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                    artist_id                album_id\n0      3AA28KZvwAUcZuOKwyblJQ  0bUTHlWbkSQysoM3VsWldT\n1      3AA28KZvwAUcZuOKwyblJQ  2dIGnmEIy1WZIcZCFSj6i8\n2      3AA28KZvwAUcZuOKwyblJQ  4V9YFKLqZ5h8nQFTvDQscC\n3      3AA28KZvwAUcZuOKwyblJQ  2dIGnmEIy1WZIcZCFSj6i8\n4      3AA28KZvwAUcZuOKwyblJQ  0YvYmLBFFwYxgI4U9KKgUm\n...                       ...                     ...\n20713  3EYY5FwDkHEYLw5V86SAtl  7o5cIYGdDmDqa9gGNsU60e\n20714  3EYY5FwDkHEYLw5V86SAtl  2JR4Wct66k7JOEh6y5yy0L\n20715  3EYY5FwDkHEYLw5V86SAtl  7v9knMsQE5CkN2HE4yhIQu\n20716  3EYY5FwDkHEYLw5V86SAtl  3xynK8Rwi02G6VKcb15rFJ\n20717  3EYY5FwDkHEYLw5V86SAtl  6Kj90CxlnIB4bYECjGnbGp\n\n[20718 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist_id</th>\n      <th>album_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3AA28KZvwAUcZuOKwyblJQ</td>\n      <td>0bUTHlWbkSQysoM3VsWldT</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3AA28KZvwAUcZuOKwyblJQ</td>\n      <td>2dIGnmEIy1WZIcZCFSj6i8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3AA28KZvwAUcZuOKwyblJQ</td>\n      <td>4V9YFKLqZ5h8nQFTvDQscC</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3AA28KZvwAUcZuOKwyblJQ</td>\n      <td>2dIGnmEIy1WZIcZCFSj6i8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3AA28KZvwAUcZuOKwyblJQ</td>\n      <td>0YvYmLBFFwYxgI4U9KKgUm</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20713</th>\n      <td>3EYY5FwDkHEYLw5V86SAtl</td>\n      <td>7o5cIYGdDmDqa9gGNsU60e</td>\n    </tr>\n    <tr>\n      <th>20714</th>\n      <td>3EYY5FwDkHEYLw5V86SAtl</td>\n      <td>2JR4Wct66k7JOEh6y5yy0L</td>\n    </tr>\n    <tr>\n      <th>20715</th>\n      <td>3EYY5FwDkHEYLw5V86SAtl</td>\n      <td>7v9knMsQE5CkN2HE4yhIQu</td>\n    </tr>\n    <tr>\n      <th>20716</th>\n      <td>3EYY5FwDkHEYLw5V86SAtl</td>\n      <td>3xynK8Rwi02G6VKcb15rFJ</td>\n    </tr>\n    <tr>\n      <th>20717</th>\n      <td>3EYY5FwDkHEYLw5V86SAtl</td>\n      <td>6Kj90CxlnIB4bYECjGnbGp</td>\n    </tr>\n  </tbody>\n</table>\n<p>20718 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# composes - isComposedBy\n",
    "composes_df = pd.DataFrame({\n",
    "    'artist_id': dataset['Url_spotify'],\n",
    "    'album_id': spotify_album['Id']\n",
    "})\n",
    "composes_df['artist_id'] = composes_df['artist_id'].apply(lambda uri: uri.split('/')[-1])\n",
    "composes_df['album_id'] = composes_df['album_id'].apply(lambda uri: uri.split(':')[-1])\n",
    "composes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:11:12.157434Z",
     "start_time": "2023-12-02T14:11:12.132434800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                    artist_id                track_id\n0      3AA28KZvwAUcZuOKwyblJQ  0d28khcov6AiegSCpG5TuT\n1      3AA28KZvwAUcZuOKwyblJQ  1foMv2HQwfQ2vntFf9HFeG\n2      3AA28KZvwAUcZuOKwyblJQ  64dLd6rVqDLtkXFYrEUHIU\n3      3AA28KZvwAUcZuOKwyblJQ  0q6LuUqGLUiCPP1cbdwFs3\n4      3AA28KZvwAUcZuOKwyblJQ  7yMiX7n9SBvadzox8T5jzT\n...                       ...                     ...\n20713  3EYY5FwDkHEYLw5V86SAtl  0RtcKQGyI4hr8FgFH1TuYG\n20714  3EYY5FwDkHEYLw5V86SAtl  3rHvPA8lUnPBkaLyPOc0VV\n20715  3EYY5FwDkHEYLw5V86SAtl  4jk00YxPtPbhvHJE9N4ddv\n20716  3EYY5FwDkHEYLw5V86SAtl  5EyErbpsugWliX006eTDex\n20717  3EYY5FwDkHEYLw5V86SAtl  6lOn0jz1QpjcWeXo1oMm0k\n\n[20718 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist_id</th>\n      <th>track_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3AA28KZvwAUcZuOKwyblJQ</td>\n      <td>0d28khcov6AiegSCpG5TuT</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3AA28KZvwAUcZuOKwyblJQ</td>\n      <td>1foMv2HQwfQ2vntFf9HFeG</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3AA28KZvwAUcZuOKwyblJQ</td>\n      <td>64dLd6rVqDLtkXFYrEUHIU</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3AA28KZvwAUcZuOKwyblJQ</td>\n      <td>0q6LuUqGLUiCPP1cbdwFs3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3AA28KZvwAUcZuOKwyblJQ</td>\n      <td>7yMiX7n9SBvadzox8T5jzT</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20713</th>\n      <td>3EYY5FwDkHEYLw5V86SAtl</td>\n      <td>0RtcKQGyI4hr8FgFH1TuYG</td>\n    </tr>\n    <tr>\n      <th>20714</th>\n      <td>3EYY5FwDkHEYLw5V86SAtl</td>\n      <td>3rHvPA8lUnPBkaLyPOc0VV</td>\n    </tr>\n    <tr>\n      <th>20715</th>\n      <td>3EYY5FwDkHEYLw5V86SAtl</td>\n      <td>4jk00YxPtPbhvHJE9N4ddv</td>\n    </tr>\n    <tr>\n      <th>20716</th>\n      <td>3EYY5FwDkHEYLw5V86SAtl</td>\n      <td>5EyErbpsugWliX006eTDex</td>\n    </tr>\n    <tr>\n      <th>20717</th>\n      <td>3EYY5FwDkHEYLw5V86SAtl</td>\n      <td>6lOn0jz1QpjcWeXo1oMm0k</td>\n    </tr>\n  </tbody>\n</table>\n<p>20718 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writes - isWrittenBy\n",
    "writes_df = pd.DataFrame({\n",
    "    'artist_id': composes_df['artist_id'],\n",
    "    'track_id': dataset['Uri'].apply(lambda uri: uri.split(':')[-1])\n",
    "})\n",
    "writes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:11:13.150984Z",
     "start_time": "2023-12-02T14:11:13.107985500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                     album_id                track_id\n0      0bUTHlWbkSQysoM3VsWldT  0d28khcov6AiegSCpG5TuT\n1      2dIGnmEIy1WZIcZCFSj6i8  1foMv2HQwfQ2vntFf9HFeG\n2      4V9YFKLqZ5h8nQFTvDQscC  64dLd6rVqDLtkXFYrEUHIU\n3      2dIGnmEIy1WZIcZCFSj6i8  0q6LuUqGLUiCPP1cbdwFs3\n4      0YvYmLBFFwYxgI4U9KKgUm  7yMiX7n9SBvadzox8T5jzT\n...                       ...                     ...\n20713  7o5cIYGdDmDqa9gGNsU60e  0RtcKQGyI4hr8FgFH1TuYG\n20714  2JR4Wct66k7JOEh6y5yy0L  3rHvPA8lUnPBkaLyPOc0VV\n20715  7v9knMsQE5CkN2HE4yhIQu  4jk00YxPtPbhvHJE9N4ddv\n20716  3xynK8Rwi02G6VKcb15rFJ  5EyErbpsugWliX006eTDex\n20717  6Kj90CxlnIB4bYECjGnbGp  6lOn0jz1QpjcWeXo1oMm0k\n\n[20718 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>album_id</th>\n      <th>track_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0bUTHlWbkSQysoM3VsWldT</td>\n      <td>0d28khcov6AiegSCpG5TuT</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2dIGnmEIy1WZIcZCFSj6i8</td>\n      <td>1foMv2HQwfQ2vntFf9HFeG</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4V9YFKLqZ5h8nQFTvDQscC</td>\n      <td>64dLd6rVqDLtkXFYrEUHIU</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2dIGnmEIy1WZIcZCFSj6i8</td>\n      <td>0q6LuUqGLUiCPP1cbdwFs3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0YvYmLBFFwYxgI4U9KKgUm</td>\n      <td>7yMiX7n9SBvadzox8T5jzT</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20713</th>\n      <td>7o5cIYGdDmDqa9gGNsU60e</td>\n      <td>0RtcKQGyI4hr8FgFH1TuYG</td>\n    </tr>\n    <tr>\n      <th>20714</th>\n      <td>2JR4Wct66k7JOEh6y5yy0L</td>\n      <td>3rHvPA8lUnPBkaLyPOc0VV</td>\n    </tr>\n    <tr>\n      <th>20715</th>\n      <td>7v9knMsQE5CkN2HE4yhIQu</td>\n      <td>4jk00YxPtPbhvHJE9N4ddv</td>\n    </tr>\n    <tr>\n      <th>20716</th>\n      <td>3xynK8Rwi02G6VKcb15rFJ</td>\n      <td>5EyErbpsugWliX006eTDex</td>\n    </tr>\n    <tr>\n      <th>20717</th>\n      <td>6Kj90CxlnIB4bYECjGnbGp</td>\n      <td>6lOn0jz1QpjcWeXo1oMm0k</td>\n    </tr>\n  </tbody>\n</table>\n<p>20718 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# containsTrack - isInAlbum\n",
    "containsTrack_df = pd.DataFrame({\n",
    "    'album_id': composes_df['album_id'],\n",
    "    'track_id': writes_df['track_id']\n",
    "})\n",
    "containsTrack_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDFLib import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To use RDFLib, the following installation is required:\n",
    "\n",
    "<code>pip3 install rdflib</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:11:41.102241100Z",
     "start_time": "2023-12-02T14:11:40.428726300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "# rdflib knows about some namespaces, like FOAF\n",
    "from rdflib.namespace import FOAF, XSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:13:22.450874300Z",
     "start_time": "2023-12-02T14:13:22.199907500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct the countries and the SoundGraph ontology namespaces not known by RDFlib\n",
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "SG = Namespace(\"https://www.dei.unipd.it/db2/ontology/soundgraph#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"sg\", SG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:13:24.536518700Z",
     "start_time": "2023-12-02T14:13:24.532503900Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_and_empty_graph(graph, filename):\n",
    "    with open(targetFolder + filename, 'w') as file:\n",
    "        file.write(graph.serialize(format='turtle'))\n",
    "        \n",
    "    graph = Graph()\n",
    "    graph.bind(\"foaf\", FOAF)\n",
    "    graph.bind(\"xsd\", XSD)\n",
    "    graph.bind(\"countries\", CNS)\n",
    "    graph.bind(\"sg\", SG)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Spotify Artist  and  hasNationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:13:28.782827600Z",
     "start_time": "2023-12-02T14:13:26.738448600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.97 s, sys: 17.5 ms, total: 1.98 s\n",
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#measure execution time\n",
    "\n",
    "#iterate over the movies dataframe\n",
    "for index, row in wikidata_artists.iterrows():\n",
    "    row2 = spotify_artist_info.iloc[index]\n",
    "    \n",
    "    # create node\n",
    "    artist_uri = \"artist_\" + row['Url_spotify']\n",
    "    Artist = URIRef(SG[artist_uri])\n",
    "    g.add((Artist, RDF.type, SG.SpotifyArtist))\n",
    "    # data properties\n",
    "    g.add((Artist, SG['artistFollowersNum'], Literal(row2['Followers'], datatype=XSD.integer)))\n",
    "    g.add((Artist, SG['artistName'], Literal(row['Artist'], datatype=XSD.string)))\n",
    "    g.add((Artist, SG['artistPopularity'], Literal(row2['Popularity'], datatype=XSD.integer)))\n",
    "    if row['websiteLabel'] != '_':\n",
    "        g.add((Artist, SG['artistWebsite'], Literal(row['websiteLabel'], datatype=XSD.string)))\n",
    "    if row['start'] != '_':\n",
    "        g.add((Artist, SG['startWorkingPeriod'], Literal(row['start'], datatype=XSD.gYear)))\n",
    "    if row['end'] != '_':\n",
    "        g.add((Artist, SG['endWorkingPeriod'], Literal(row['end'], datatype=XSD.gYear)))\n",
    "    if row['dissolved'] != '_':\n",
    "        g.add((Artist, SG['dissolvedIn'], Literal(row['dissolved'], datatype=XSD.gYear)))\n",
    "    # obj prop - hasNationality\n",
    "    if row['country_codes'] != '_':\n",
    "        cc_list = row['country_codes'].split('+')\n",
    "        for cc in cc_list:\n",
    "            Country = URIRef(CNS[cc.lower()])\n",
    "            g.add((Artist, SG['hasNationality'], Country))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:13:30.641300900Z",
     "start_time": "2023-12-02T14:13:30.232258100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 315 ms, sys: 0 ns, total: 315 ms\n",
      "Wall time: 330 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "g = write_and_empty_graph(g, 'spotify_artist.ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Spotify Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:13:47.168966Z",
     "start_time": "2023-12-02T14:13:37.905837800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.12 s, sys: 102 ms, total: 9.22 s\n",
      "Wall time: 9.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset['Stream'] = dataset['Stream'].fillna('_')\n",
    "for index, row in dataset.iterrows():\n",
    "    \n",
    "    # Create the node to add to the Graph\n",
    "    track_uri = \"track_\" + row['Uri'].split(':')[-1]\n",
    "    Track = URIRef(SG[track_uri])\n",
    "    # Add triples using store's add() method.\n",
    "    g.add((Track, RDF.type, SG.SpotifyTrack))\n",
    "    g.add((Track, SG['trackName'], Literal(row['Track'], datatype=XSD.string)))\n",
    "    if not pd.isna(row['Duration_ms']):\n",
    "        g.add((Track, SG['trackAcousticness'], Literal(row['Acousticness'], datatype=XSD.float)))\n",
    "        g.add((Track, SG['trackDanceability'], Literal(row['Danceability'], datatype=XSD.float)))\n",
    "        g.add((Track, SG['trackDuration'], Literal(row['Duration_ms'], datatype=XSD.integer)))\n",
    "        g.add((Track, SG['trackEnergy'], Literal(row['Energy'], datatype=XSD.float)))\n",
    "        g.add((Track, SG['trackInstrumentalness'], Literal(row['Instrumentalness'], datatype=XSD.float)))    \n",
    "        g.add((Track, SG['trackKey'], Literal(row['Key'], datatype=XSD.integer)))\n",
    "        g.add((Track, SG['trackLiveness'], Literal(row['Liveness'], datatype=XSD.float)))\n",
    "        g.add((Track, SG['trackLoudness'], Literal(row['Loudness'], datatype=XSD.float)))\n",
    "        g.add((Track, SG['trackSpeechiness'], Literal(row['Speechiness'], datatype=XSD.float)))\n",
    "        g.add((Track, SG['trackTempo'], Literal(row['Tempo'], datatype=XSD.float)))\n",
    "        g.add((Track, SG['trackValence'], Literal(row['Valence'], datatype=XSD.float)))\n",
    "    if row['Stream'] != '_':\n",
    "        g.add((Track, SG['trackStreams'], Literal(row['Stream'], datatype=XSD.integer)))   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:13:54.733026400Z",
     "start_time": "2023-12-02T14:13:54.574399700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 99.6 ms, sys: 0 ns, total: 99.6 ms\n",
      "Wall time: 98.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "genres_set = set()\n",
    "spotify_artist_info['Genres'] = spotify_artist_info['Genres'].apply(lambda x: x.replace('[', '')\n",
    "                                                                            .replace(']', '')\n",
    "                                                                            .replace('\\'','')\n",
    "                                                                            .replace('\"',''))\n",
    "for index, row in spotify_artist_info.iterrows():\n",
    "    for genre in row['Genres'].split(', '):\n",
    "        # some songs have no associated genres and the replace series results in an empty string\n",
    "        if genre != '':\n",
    "            genres_set.add(genre.replace(' ', '_').replace('-', '_'))\n",
    "\n",
    "for genre in genres_set:\n",
    "    genre_uri = \"genre_\" + genre\n",
    "    Genre = URIRef(SG[genre_uri])\n",
    "    g.add((Genre, RDF.type, SG.Genre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Spotify Album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:14:03.740434300Z",
     "start_time": "2023-12-02T14:14:00.895994700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.75 s, sys: 28.5 ms, total: 2.77 s\n",
      "Wall time: 2.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# add album type to CSV and remove duplicates in another var\n",
    "spotify_album['album_type'] = dataset['Album_type']\n",
    "spotify_album_2 = spotify_album.drop_duplicates().reset_index(drop=True)\n",
    "spotify_album_2['Album'] = spotify_album_2['Album'].fillna('_')\n",
    "\n",
    "for index, row in spotify_album_2.iterrows():\n",
    "    album_uri = 'album_' + row['Id'].split(':')[-1]\n",
    "    if row['Album'] != '_':\n",
    "        Album = URIRef(SG[album_uri])\n",
    "        g.add((Album, RDF.type, SG.SpotifyAlbum))\n",
    "        g.add((Album, SG['albumName'], Literal(row['Album'], datatype=XSD.string)))\n",
    "        g.add((Album, SG['albumReleaseDate'], Literal(row['Release_date'], datatype=XSD.date)))\n",
    "        g.add((Album, SG['albumTotalTracksNum'], Literal(row['Total_tracks'], datatype=XSD.integer)))\n",
    "        g.add((Album, SG['albumType'], Literal(row['album_type'], datatype=XSD.string))) # TODO check enum datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:14:13.505930500Z",
     "start_time": "2023-12-02T14:14:05.240332500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.16 s, sys: 26.6 ms, total: 8.19 s\n",
      "Wall time: 8.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "g = write_and_empty_graph(g, 'nodes_spotify.ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### YouTube Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:14:19.483929900Z",
     "start_time": "2023-12-02T14:14:18.267231500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.14 s, sys: 20 ms, total: 1.16 s\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "youtube_api_channels['channelDescription'] = youtube_api_channels['channelDescription'].fillna('_')\n",
    "for index, row in youtube_api_channels.iterrows():\n",
    "    if row['channelId'] != '_':\n",
    "        channel_id = 'channel_' + row['channelId']\n",
    "        Channel = URIRef(SG[channel_id])\n",
    "        g.add((Channel, RDF.type, SG.YouTubeChannel))\n",
    "        g.add((Channel, SG['channelName'], Literal(row['originalChannel'], datatype=XSD.string)))\n",
    "        if row['channelDescription'] != '_':\n",
    "            g.add((Channel, SG['channelDescription'], Literal(row['channelDescription'], datatype=XSD.string)))\n",
    "        g.add((Channel, SG['channelViewCount'], Literal(row['viewCount'], datatype=XSD.integer)))\n",
    "        g.add((Channel, SG['channelSubscribersCount'], Literal(row['subscriberCount'], datatype=XSD.integer)))\n",
    "        g.add((Channel, SG['channelVideoCount'], Literal(row['videoCount'], datatype=XSD.integer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:14:22.174225400Z",
     "start_time": "2023-12-02T14:14:20.701929600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.38 s, sys: 0 ns, total: 1.38 s\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "g = write_and_empty_graph(g, 'nodes_yt_channels.ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:14:30.158770200Z",
     "start_time": "2023-12-02T14:14:23.015297800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.05 s, sys: 78.3 ms, total: 7.13 s\n",
      "Wall time: 7.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset['Url_youtube'] = dataset['Url_youtube'].fillna('_')\n",
    "dataset['Description'] = dataset['Description'].fillna('_')\n",
    "dataset['Comments'] = dataset['Comments'].fillna(-1)\n",
    "dataset['Likes'] = dataset['Likes'].fillna(-1)\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    if row['Url_youtube'] != '_':\n",
    "        video_id = 'video_' + row['Url_youtube'].split('?v=')[-1]\n",
    "        Video = URIRef(SG[video_id])\n",
    "        g.add((Video, RDF.type, SG.YouTubeVideo))\n",
    "        g.add((Video, SG['videoTitle'], Literal(row['Title'], datatype=XSD.string)))\n",
    "        if row['Description'] != '_':\n",
    "            g.add((Video, SG['videoDescription'], Literal(row['Description'], datatype=XSD.string)))\n",
    "        if row['Comments'] != -1:\n",
    "            g.add((Video, SG['videoComments'], Literal(row['Comments'], datatype=XSD.integer)))\n",
    "        if row['Likes'] != -1:\n",
    "            g.add((Video, SG['videoLikes'], Literal(row['Likes'], datatype=XSD.integer)))\n",
    "        g.add((Video, SG['videoViews'], Literal(row['Views'], datatype=XSD.integer)))\n",
    "        g.add((Video, SG['isOfficialVideo'], Literal(row['official_video'], datatype=XSD.boolean)))\n",
    "        g.add((Video, SG['isLicensed'], Literal(row['Licensed'], datatype=XSD.boolean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:14:33.683407200Z",
     "start_time": "2023-12-02T14:14:30.149772400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.4 s, sys: 54.8 ms, total: 3.46 s\n",
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "g = write_and_empty_graph(g, 'nodes_yt_video.ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Object properties\n",
    "#### hasNationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:14:43.273229Z",
     "start_time": "2023-12-02T14:14:43.264229100Z"
    }
   },
   "outputs": [],
   "source": [
    "#Already done in Spotify Artist "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### containsTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:16:02.008823100Z",
     "start_time": "2023-12-02T14:16:00.262681400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.66 s, sys: 29.7 ms, total: 1.69 s\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# add album type to CSV and remove duplicates in another var\n",
    "dataset['Album_Id'] = spotify_album['Id']\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    album_uri = 'album_' + row['Album_Id'].split(':')[-1]\n",
    "    Album = URIRef(SG[album_uri])\n",
    "\n",
    "    track_uri = \"track_\" + row['Uri'].split(':')[-1]\n",
    "    Track = URIRef(SG[track_uri])\n",
    "        \n",
    "    g.add((Album, SG['containsTrack'], Track))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### isRelatedTo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:16:14.295343300Z",
     "start_time": "2023-12-02T14:16:12.419478100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.8 s, sys: 0 ns, total: 1.8 s\n",
      "Wall time: 1.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    if row['Url_youtube'] != '_' and row['Uri'] != '_':\n",
    "        video_id = 'video_' + row['Url_youtube'].split('?v=')[-1]\n",
    "        Video = URIRef(SG[video_id])\n",
    "\n",
    "        track_uri = \"track_\" + row['Uri'].split(':')[-1]\n",
    "        Track = URIRef(SG[track_uri])\n",
    "        \n",
    "        g.add((Video, SG['isRelatedTo'], Track))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### isPublishedBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "youtube_dataset = dataset[['Url_youtube', 'Channel']].copy()\n",
    "\n",
    " #Rimuovi i duplicati\n",
    "youtube_dataset.drop_duplicates().reset_index(drop=True)\n",
    "youtube_dataset['Url_youtube'] = youtube_dataset['Url_youtube'].fillna('')\n",
    "youtube_dataset['Channel'] = youtube_dataset['Channel'].fillna('')\n",
    "\n",
    "for index, row in youtube_dataset.iterrows():\n",
    "    if row['Url_youtube'] != '':\n",
    "\n",
    "        video_id = 'video' + row['Url_youtube'].split('?v=')[-1]\n",
    "        Video = URIRef(SG[video_id])\n",
    "\n",
    "        for idx, channel in youtube_api_channels.iterrows():\n",
    "            if channel['title'] == row['Channel']:\n",
    "                channel_id = 'channel' + channel['channelId']\n",
    "                Channel = URIRef(SG[channel_id])\n",
    "\n",
    "                g.add((Video, SG['isPublishedBy'], Channel))\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### isAvailableIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:16:23.116503400Z",
     "start_time": "2023-12-02T14:16:22.218197700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 825 ms, sys: 0 ns, total: 825 ms\n",
      "Wall time: 824 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# add album type to CSV and remove duplicates in another var\n",
    "spotify_album_2 = spotify_album.drop_duplicates().reset_index(drop=True)\n",
    "spotify_album_2['Album'] = spotify_album_2['Album'].fillna('_')\n",
    "\n",
    "for index, row in spotify_album_2.iterrows():\n",
    "    album_uri = 'album_' + row['Id'].split(':')[-1]\n",
    "    if row['Album'] != '_':\n",
    "        Album = URIRef(SG[album_uri])\n",
    "        \n",
    "        if isinstance(row['Available_market'], list) and len(row['Available_market']) > 0:\n",
    "            for market in row['Available_market']:\n",
    "                if market != '_' and len(market) == 2:\n",
    "                    Country = URIRef(CNS[market.lower()])\n",
    "                    g.add((Album, SG['isAvailableIn'], Country))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### isOfficialChannel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### isWrittenBy  and  performsIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:17:55.073237300Z",
     "start_time": "2023-12-02T14:17:53.208790400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.77 s, sys: 19.5 ms, total: 1.79 s\n",
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset['Url_youtube'] = dataset['Url_youtube'].fillna('_')\n",
    "        \n",
    "for index, row in dataset.iterrows():\n",
    "\n",
    "    artist_uri = \"artist_\" + row['Url_spotify'].split('/')[-1]\n",
    "    Artist = URIRef(SG[artist_uri])\n",
    "    \n",
    "    track_uri = \"track_\" + row['Uri'].split(':')[-1]\n",
    "    Track = URIRef(SG[track_uri])\n",
    "    \n",
    "    #   isWrittenBy\n",
    "    g.add((Track, SG['isWrittenBy'], Artist))\n",
    "\n",
    "    #   performIn\n",
    "    if row['Url_youtube'] != '_':\n",
    "        video_id = 'video_' + row['Url_youtube'].split('?v=')[-1]\n",
    "        Video = URIRef(SG[video_id])\n",
    "        \n",
    "        g.add((Artist, SG['performsIn'], Video))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### hasGenre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:39:04.920902900Z",
     "start_time": "2023-12-02T14:39:04.709794600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 160 ms, sys: 0 ns, total: 160 ms\n",
      "Wall time: 159 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spotify_artist_info['artist_id'] = wikidata_artists['Url_spotify']\n",
    "for index, row in spotify_artist_info.iterrows():\n",
    "    artist_uri = \"artist_\" + row['artist_id']\n",
    "    Artist = URIRef(SG[artist_uri])\n",
    "    \n",
    "    for genre in row['Genres'].split(', '):\n",
    "        if genre != '':\n",
    "            genre_uri = \"genre_\" + genre.replace(' ', '_').replace('-', '_')\n",
    "            Genre = URIRef(SG[genre_uri])\n",
    "            \n",
    "            g.add((Artist, SG['hasGenre'], Genre))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### isComposedBy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T14:38:14.860153800Z",
     "start_time": "2023-12-02T14:38:13.940219700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 866 ms, sys: 0 ns, total: 866 ms\n",
      "Wall time: 865 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# add album type to CSV and remove duplicates in another var\n",
    "spotify_album['artist_id'] = dataset['Url_spotify']\n",
    "spotify_album_2 = spotify_album.drop_duplicates().reset_index(drop=True)\n",
    "spotify_album_2['Album'] = spotify_album_2['Album'].fillna('_')\n",
    "\n",
    "for index, row in spotify_album_2.iterrows():\n",
    "    album_uri = 'album_' + row['Id'].split(':')[-1]\n",
    "    Album = URIRef(SG[album_uri])\n",
    "    \n",
    "    artist_uri = \"artist_\" + row['artist_id'].split(':')[-1]\n",
    "    Artist = URIRef(SG[artist_uri])\n",
    "    \n",
    "    g.add((Album, SG['isComposedBy'], Artist))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "db2",
   "language": "python",
   "display_name": "db2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
