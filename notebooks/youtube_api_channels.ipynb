{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8a25b8-1986-4a44-aefe-4d1662c4191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee2891b-d302-4154-b826-d3e7c8603bcd",
   "metadata": {},
   "source": [
    "## YouTube API service\n",
    "First we need to create a service from which we can access the APIs and make requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271008c2-1712-4fb0-ae54-1c15c84a27b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of Youtube Data API service\n",
    "api_key = 'AIzaSyCu2QSo3aGQfN6O003YulbSGsJThjD8i4k'\n",
    "api_version = 'v3'\n",
    "service_name = 'youtube'\n",
    "\n",
    "youtube_service = build(service_name, api_version, developerKey=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a89703-3290-4f44-b69c-933931896b9a",
   "metadata": {},
   "source": [
    "These two functions allow to find the channel ID from a video, and then to get channel info such as:\n",
    "- channel name\n",
    "- channel description\n",
    "- channel view count\n",
    "- channel video count\n",
    "- channel subscriber count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "928a3003-eca1-4eb7-a873-4c579396a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that finds channel id from video\n",
    "def get_channel_id(service, video_id):\n",
    "    request = service.videos().list(\n",
    "        part = 'snippet',\n",
    "        id = video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    if not len(response['items']):\n",
    "        return None\n",
    "        \n",
    "    channel_id = response['items'][0]['snippet']['channelId']\n",
    "    return channel_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8813ac6a-b113-4009-868d-4bde7080c715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that retrieves info about the channel\n",
    "def get_channel_data(service, channel_id):\n",
    "    request = service.channels().list(\n",
    "        part = 'snippet, statistics',\n",
    "        id = channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    if 'items' not in response.keys():\n",
    "        return None\n",
    "        \n",
    "    response_items = response['items'][0]\n",
    "    channel_info = {\n",
    "        'channelId': channel_id,\n",
    "        'title': response_items['snippet']['title'],\n",
    "        'channelDescription': response_items['snippet']['description'],\n",
    "        'viewCount': response_items['statistics']['viewCount'],\n",
    "        'subscriberCount': response_items['statistics']['subscriberCount'],\n",
    "        'videoCount': response_items['statistics']['videoCount'],\n",
    "    }\n",
    "\n",
    "    return channel_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1a41b-1024-4c36-9605-b8d770122bbd",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- read the .csv file (only ```Url_youtube``` and ```Channel``` columns)\n",
    "- drop duplicated channels: the data we ask for will be the same every time\n",
    "- substitute NaN values\n",
    "- extract video ID from the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf823b3-8c06-47f4-84ca-14517b5e1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset and keep only relevant info for this task\n",
    "video_channel_df = pd.read_csv('../data/Spotify_Youtube.csv', usecols=['Url_youtube', 'Channel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74234a48-2606-43b0-baf6-1c72227d2212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates on channel\n",
    "video_channel_df = video_channel_df.drop_duplicates(subset='Channel').reset_index(drop=True)\n",
    "# fill NaN to avoid error between expected type str and actual type float\n",
    "video_channel_df = video_channel_df.fillna('_')\n",
    "# extract video id from URL\n",
    "video_channel_df['Url_youtube'] = video_channel_df['Url_youtube'].apply(lambda x: x.split('?v=')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d013dee-9444-481b-95ab-35f693e53405",
   "metadata": {},
   "source": [
    "## API quota\n",
    "YouTube Data API allows for max 10k requests per day.\\\n",
    "After the ```drop_duplicates```, this dataset has 6715 rows. Since we need 2 requests per row (video and channel), we need to split the data into 2 parts (and run the following function in two separate days...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf01b81-538b-4430-a3ed-32a0d91a7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "half_rows = int(len(video_channel_df) / 2)\n",
    "video_channel_df_parts = [video_channel_df.iloc[:half_rows, :], video_channel_df.iloc[half_rows:, :]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239aa8e9-ea01-4f04-99ee-df7a4c7229df",
   "metadata": {},
   "source": [
    "## Save .csv part file\n",
    "This function saves data about a part of the dataset.\\\n",
    "In the code we take note of 3 types of anomalies:\n",
    "1. ```error = 0``` indicates no errors\n",
    "2. ```error = 1``` indicates rows for which we don't have Youtube data in our dataset\n",
    "3. ```error = 2``` indicates rows for which we have YouTube data, but the video has been removed (recall that this dataset refers to Feb '23)\n",
    "4. ```error = 3``` indicates rows for which we have YouTube data, the video exists but is not associated with any channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85cf8e8c-782a-4d71-a916-2c4d28d2fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv_part(service, parts, part_id):\n",
    "    # lists to store data\n",
    "    channel_ids = []\n",
    "    title = []\n",
    "    channel_description = []\n",
    "    view_count = []\n",
    "    subscriber_count = []\n",
    "    video_count = []\n",
    "    errors = []\n",
    "\n",
    "    partition = parts[part_id]\n",
    "    \n",
    "    for index, row in tqdm(partition.iterrows(), total=partition.shape[0]):\n",
    "        # default: error 1\n",
    "        current_channel_id = '_'\n",
    "        current_title = '_'\n",
    "        current_channel_description = '_'\n",
    "        current_view_count = '_'\n",
    "        current_subscriber_count = '_'\n",
    "        current_video_count = '_'\n",
    "        error = 1\n",
    "\n",
    "        # if we have YouTube data\n",
    "        if row['Url_youtube'] != '_':\n",
    "            # retrieve channel id\n",
    "            channel_id = get_channel_id(service, row['Url_youtube'])\n",
    "            # if there is channel id\n",
    "            if channel_id is not None:\n",
    "                # retrieve channel data\n",
    "                channel_info = get_channel_data(service, channel_id)\n",
    "                # if there is channel data\n",
    "                if channel_info is not None:\n",
    "                    current_channel_id = channel_info['channelId']\n",
    "                    current_title = channel_info['title']\n",
    "                    current_channel_description = channel_info['channelDescription']\n",
    "                    current_view_count = channel_info['viewCount']\n",
    "                    current_subscriber_count = channel_info['subscriberCount']\n",
    "                    current_video_count = channel_info['videoCount']\n",
    "                    error = 0\n",
    "                else:\n",
    "                    # if no channel data -> err 3\n",
    "                    current_channel_id = '_'\n",
    "                    current_title = '_'\n",
    "                    current_channel_description = '_'\n",
    "                    current_view_count = '_'\n",
    "                    current_subscriber_count = '_'\n",
    "                    current_video_count = '_'\n",
    "                    error = 3\n",
    "            else:\n",
    "                # if there is no video -> err 2\n",
    "                current_channel_id = '_'\n",
    "                current_title = '_'\n",
    "                current_channel_description = '_'\n",
    "                current_view_count = '_'\n",
    "                current_subscriber_count = '_'\n",
    "                current_video_count = '_'\n",
    "                error = 2\n",
    "\n",
    "        # append the result\n",
    "        channel_ids.append(current_channel_id)\n",
    "        title.append(current_title)\n",
    "        channel_description.append(current_channel_description)\n",
    "        view_count.append(current_view_count)\n",
    "        subscriber_count.append(current_subscriber_count)\n",
    "        video_count.append(current_video_count)\n",
    "        errors.append(error)\n",
    "\n",
    "    # create new dataframe to store all the data about channels and save it in a csv file\n",
    "    channels = pd.DataFrame({\n",
    "        'channelId': channel_ids,\n",
    "        'title': title,\n",
    "        'channelDescription': channel_description,\n",
    "        'viewCount': view_count,\n",
    "        'subscriberCount': subscriber_count,\n",
    "        'videoCount': video_count,\n",
    "        'error': errors\n",
    "    })\n",
    "    channels.to_csv(f'../data/youtubeapi_channels_part{part_id + 1}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a8336f-f5d9-446b-97da-efb94f92dbc0",
   "metadata": {},
   "source": [
    "## Save the data\n",
    "Run the first function call, then wait for the quota update to be able to run the second function call. See here for [quota details](https://developers.google.com/youtube/v3/determine_quota_cost).\n",
    "\n",
    "Quota should reset at midnight Pacific Time, so 8 AM in Rome time zone.\\\n",
    "I can't find this specification in the YouTube Data API documentation, but I found [this error message](https://stackoverflow.com/questions/61512256/youtube-data-api-10-000-quota-reached-with-just-a-few-hundred-put-updates). Now the message only tells users that they have exceeded quota, without mentioning when it will resets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f681ba4-4997-4665-a141-3746344e38db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 3357/3357 [05:00<00:00, 11.17it/s]\n"
     ]
    }
   ],
   "source": [
    "save_csv_part(youtube_service, video_channel_df_parts, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47bbd290-cfce-4b4c-b4b7-2c7dd5f47a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 3358/3358 [04:48<00:00, 11.62it/s]\n"
     ]
    }
   ],
   "source": [
    "save_csv_part(youtube_service, video_channel_df_parts, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e816c65-d07d-49b3-a24d-1a84a957bdb8",
   "metadata": {},
   "source": [
    "## Merge into one single file\n",
    "Now that we have both .csv files we can concat them, save a single .csv file and delete the part files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3b19059-0192-4392-9f91-543d64d0e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "part1 = pd.read_csv('../data/youtubeapi_channels_part1.csv')\n",
    "part2 = pd.read_csv('../data/youtubeapi_channels_part2.csv')\n",
    "\n",
    "part1['originalChannel'] = video_channel_df_parts[0]['Channel']\n",
    "part2['originalChannel'] = video_channel_df_parts[1]['Channel']\n",
    "\n",
    "complete_df = pd.concat([part1, part2])\n",
    "complete_df.to_csv('../data/youtubeapi_channels_complete.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd60287c-ce88-4c81-882c-86cd8061f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('../data/youtubeapi_channels_part1.csv'):\n",
    "    os.remove('../data/youtubeapi_channels_part1.csv')\n",
    "if os.path.exists('../data/youtubeapi_channels_part2.csv'):\n",
    "    os.remove('../data/youtubeapi_channels_part2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db2",
   "language": "python",
   "name": "db2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
